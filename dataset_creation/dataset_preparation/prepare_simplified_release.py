"""
A script to prepare a simplified release of the AI-GenBench dataset.

The intermediate AI-GenBench dataset obtained by using main_dataset_generation also contains generators that are not used in the
benchmark. In addition, due to internal processing needs, it can't be loaded without importing and registering the LargeImage feature.

This script will strip images that are not generated by the AI-GenBench generators, and will finalize the dataset if needed.
This will also adjust the number of real samples to match the number of fake samples (note: this balancing it's otherwise done
in the training codebase, which means that having surplus real samples taking space on disk it's not really necessary).
"""

from pathlib import Path
from typing import Iterable
import random
import json
from datasets import (
    DatasetDict,
    Dataset,
    load_from_disk,
    concatenate_datasets,
)

from dataset_preparation.ai_gen_bench_generators import AI_GENBENCH_GENERATORS
from dataset_preparation.finalize_dataset import finalize_dataset, needs_finalization



def filter_fake_dataset(
    dataset: DatasetDict, generators_list: Iterable[str] = AI_GENBENCH_GENERATORS
):
    """
    Filter the dataset to only include images generated by the given list of generators.

    By default, it uses the AI-GenBench v1.0 generators list.

    Args:
        dataset (DatasetDict): The dataset to be filtered.
        generators_list (Iterable[str]): The list of generators to filter the dataset by.

    Returns:
        dataset: The filtered dataset.
    """
    generators_set = set(generators_list)

    assert len(generators_set) > 0, "The generators list must not be empty."

    result_dataset = dict()
    for split in dataset:
        print(f"Processing split: {split}, which has {len(dataset[split])} samples.")

        selected_indices = [
            i
            for i, generator_name in enumerate(dataset[split]["generator"])
            if generator_name in generators_set
        ]

        print(
            f"Selected {len(selected_indices)} samples from {len(dataset[split])} based on generators."
        )
        assert (
            len(selected_indices) > 0
        ), f"No samples found for split {split} with the specified generators."

        filtered_dataset = dataset[split].select(selected_indices)

        result_dataset[split] = filtered_dataset

    print(f"Filtered dataset contains {len(result_dataset)} splits.")

    result_dataset = DatasetDict(result_dataset)

    if needs_finalization(result_dataset):
        print("Finalizing the dataset...")
        result_dataset = finalize_dataset(result_dataset)

    return result_dataset


def filter_real_dataset(
    dataset: DatasetDict,
    selection_seed: int = 42,
    n_generators: int = len(AI_GENBENCH_GENERATORS),
) -> DatasetDict:
    """
    Filter the dataset to have a balanced number of real images.
    """

    assert n_generators > 0, "The number of generators must be greater than 0."
    random.seed(selection_seed)

    result_dataset = dict()

    for split in sorted(dataset.keys()):
        dataset_split = dataset[split]
        real_images_indices = list(
            [i for i, gen in enumerate(dataset_split["generator"]) if gen == ""]
        )

        random.shuffle(real_images_indices)

        n_to_select = 4000 * n_generators if split == "train" else 1000 * n_generators

        selected_real_images = list(real_images_indices[:n_to_select])
        assert (
            len(selected_real_images) == n_to_select
        ), f"Not enough real images in split {split}. Expected {n_to_select}, found {len(selected_real_images)}."

        real_images_indices = real_images_indices[n_to_select:]

        print(f"Selecting {n_to_select} real images for split {split}.")
        selected_real_dataset = dataset_split.select(selected_real_images)
        result_dataset[split] = selected_real_dataset

    print(f"Filtered real dataset contains {len(result_dataset)} splits.")

    result_dataset = DatasetDict(result_dataset)

    if needs_finalization(result_dataset):
        print("Finalizing the real dataset...")
        result_dataset = finalize_dataset(result_dataset)

    return result_dataset


def check_ai_gen_bench_num_samples(dataset: DatasetDict):
    """
    A very specific sanity check for the AI-GenBench dataset.
    """
    for split in dataset:
        generator_labels = dataset[split]["generator"]
        n_fake_images = 0
        for generator in AI_GENBENCH_GENERATORS:
            element_indices = [
                i
                for i, generator_name in enumerate(generator_labels)
                if generator_name == generator
            ]
            n_fake_images += len(element_indices)

            assert (
                len(element_indices) == 4000 if split == "train" else 1000
            ), f"Split {split} for generator {generator} has {len(element_indices)} samples instead of the expected {4000 if split == 'train' else 1000}."

        n_real_images = len(
            [
                i
                for i, generator_name in enumerate(generator_labels)
                if generator_name == ""
            ]
        )
        assert (
            n_real_images == n_fake_images
        ), f"Split {split} has {n_real_images} real images, but expected {n_fake_images} to match the number of fake images."


def prepare_simplified_release(dataset: DatasetDict):
    """
    Prepare a simplified release of the dataset by filtering it to only include the AI-GenBench generators.

    This will filter both the fake and real parts of the dataset, ensuring that only the specified generators are included.

    Args:
        dataset (DatasetDict): The dataset to be prepared.

    Returns:
        DatasetDict: The filtered dataset containing only the specified generators.
    """
    print("Preparing simplified release of the dataset...")

    filtered_fake_part = filter_fake_dataset(dataset)
    filtered_real_part = filter_real_dataset(dataset)

    print("Combining filtered fake and real parts of the dataset...")
    filtered_dataset = DatasetDict(
        {
            "train": concatenate_datasets(
                [filtered_fake_part["train"], filtered_real_part["train"]]
            ),
            "validation": concatenate_datasets(
                [filtered_fake_part["validation"], filtered_real_part["validation"]]
            ),
        }
    )

    print("Checking the number of samples in the filtered dataset...")
    check_ai_gen_bench_num_samples(filtered_dataset)

    print("Simplified release preparation completed.")

    return filtered_dataset


def make_laion400m_filelist_from_elsa(
    final_dataset: DatasetDict,
    elsa_dataset: DatasetDict,
    spare_images: float = 0.3,
    seed: int = 1337,
):
    split_filelists = dict()

    id_to_metadata_mapping = dict()

    for split in ["train", "validation"]:
        dataset_split: Dataset = elsa_dataset[split]
        dataset_split = dataset_split.select_columns(["id", "url", "original_prompt"])

        for row in dataset_split:
            file_id = str(row["id"])
            url = row["url"]
            description = row["original_prompt"]
            assert (
                file_id not in id_to_metadata_mapping
            ), f"Duplicate file_id {file_id} found in ELSA dataset."
            id_to_metadata_mapping[file_id] = (url, description)

    for split in final_dataset:
        file_ids = []
        file_metadata = []
        if split not in ["train", "validation"]:
            raise ValueError(
                f"Unexpected split {split} in the dataset. Expected 'train' or 'validation'."
            )

        for file_id in final_dataset[split]["file_id"]:
            if file_id.startswith("LAION-400M/"):
                # This is a LAION-400M file id
                file_id = file_id.removeprefix("LAION-400M/")

                file_ids.append(file_id)
                file_metadata.append(id_to_metadata_mapping[file_id])
                del id_to_metadata_mapping[file_id]

        # Each row must be {'url': 'http://example.com/image.jpg', "description": "...", "id": "<laion_id>"}
        urls = [
            {"url": file_metatada[0], "description": file_metatada[1], "id": file_id}
            for file_metatada, file_id in zip(file_metadata, file_ids)
        ]

        # Make json of urls
        split_filelists[split] = urls

    # Add spare images
    for split in sorted(final_dataset.keys()):
        assert split in split_filelists, f"Split {split} not found in split_filelists."
        n_spare_images = int(len(final_dataset[split]) * spare_images)
        print(f"Adding {n_spare_images} spare images to the {split} split.")

        remaining_items = list(id_to_metadata_mapping.items())

        random.seed(seed)

        for _ in range(n_spare_images):
            # Randomly select a file id and url from the remaining id_to_url_mapping
            if not id_to_metadata_mapping:
                print("No more spare images available.")
                break

            random_idx = random.randint(0, len(remaining_items) - 1)
            file_id, file_metatada = remaining_items.pop(random_idx)

            urls = [
                {
                    "url": file_metatada[0],
                    "description": file_metatada[1],
                    "id": file_id,
                }
            ]
            split_filelists[split].extend(urls)
            del id_to_metadata_mapping[file_id]

    return split_filelists


def save_fake_part_only(dataset: DatasetDict, output_path: str):
    """
    Save only the fake part of the dataset to the specified output path.

    Args:
        dataset (DatasetDict): The dataset containing both fake and real parts.
        output_path (str): The path where the fake part of the dataset will be saved.
    """

    result_dataset = dict()
    for split in dataset:
        print(f"Processing split: {split}, which has {len(dataset[split])} samples.")

        # Filter the dataset to only include fake samples (label == 1)
        selected_indices = [
            i for i, label in enumerate(dataset[split]["label"]) if label == 1
        ]

        print(
            f"Selected {len(selected_indices)} fake samples from {len(dataset[split])} total samples."
        )
        assert len(selected_indices) > 0, f"No fake samples found in split {split}."

        filtered_dataset = dataset[split].select(selected_indices)

        result_dataset[split] = filtered_dataset

    print(f"Filtered dataset contains {len(result_dataset)} splits.")
    result_dataset = DatasetDict(result_dataset)
    print(f"Saving the filtered dataset to {output_path}...")
    result_dataset.save_to_disk(output_path)


def main(
    dataset_path: str = "/home/lorenzo/deepfake_fast/ai_gen_bench_v1.0.0",
    output_path: str = "/datasets/users_working_dir/lorenzo/simplified_ai_genbench_fake_and_real",
    fake_output_path: str = "/datasets/users_working_dir/lorenzo/simplified_ai_genbench_fake",
    elsa_d3_path: str = "/datasets_nas/deepfake_datasets/ELSA_D3_offline",
):
    simplified_dataset: DatasetDict
    if Path(output_path).exists():
        print("Dataset already exists at the output path. Will use it.")
        simplified_dataset = load_from_disk(output_path)
        check_ai_gen_bench_num_samples(simplified_dataset)
    else:
        dataset = load_from_disk(dataset_path)
        print(f"Loaded dataset from {dataset_path}. It contains {len(dataset)} splits.")

        simplified_dataset = prepare_simplified_release(dataset)

        print(f"Saving the simplified dataset to {output_path}...")
        simplified_dataset.save_to_disk(output_path)
        print(f"Simplified dataset saved successfully to {output_path}.")

    if Path(fake_output_path).exists():
        print("Fake part already exists at the fake output path.")
    else:
        print(f"Saving only the fake part of the dataset to {fake_output_path}...")
        save_fake_part_only(simplified_dataset, fake_output_path)
        print(f"Fake part saved successfully to {fake_output_path}.")

    # Saving the file ids
    for split in simplified_dataset:
        print(f"Saving file ids for split {split}...")
        for label, label_type in zip([0, 1], ["real", "fake"]):
            list_path = Path(output_path) / f"{split}_{label_type}_file_ids.txt"
            with open(list_path, "w") as f:
                for file_id, sample_type in zip(
                    simplified_dataset[split]["file_id"],
                    simplified_dataset[split]["label"],
                ):
                    if sample_type == label:
                        f.write(f"{file_id}\n")
        print(f"File ids for split {split} saved to {list_path}.")

    # Generate LAION400M metadata
    print("Generating LAION400M metadata...")
    elsa_dataset: DatasetDict = load_from_disk(elsa_d3_path)
    filelists = make_laion400m_filelist_from_elsa(simplified_dataset, elsa_dataset)

    for split, filelist in filelists.items():
        filelist_path = Path(output_path) / f"{split}_laion400m_filelist.json"
        filelist_path.parent.mkdir(parents=True, exist_ok=True)
        with open(filelist_path, "w") as f:
            json.dump(filelist, f)
        print(f"LAION400M filelist for {split} saved to {filelist_path}.")

    return 0


if __name__ == "__main__":
    main()
