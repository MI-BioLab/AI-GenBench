seed_everything: true
trainer:
  strategy: "ddp_find_unused_parameters_true"
  precision: "32-true"  # You can change this to "bf16-mixed" to use mixed precision
  accumulate_grad_batches: 8
  gradient_clip_val: 1
  gradient_clip_algorithm: null
  # IMPORTANT: consider that, the the augmentation_pipelines YAML files,
  # an augmentation_factor > 1 is used, which means that the number of
  # samples in the training set is multiplied by that factor. Keep this in mind
  # when setting the number of epochs.
  max_epochs: 1
  min_epochs: 1
  logger:
    - init_args:
        save_dir: "./experiments_logs/dinov2_probe_resize"
    - init_args:
        save_dir: "./experiments_logs/wandb_logs"
data:
  class_path: lightning_data_modules.AIGenBenchPipeline
  init_args:
    # Those are the per-GPU batch sizes.
    # Consider that and the accumulate_grad_batches value above!
    train_batch_size: 64
    eval_batch_size: 64
    train_subset_size: 0  # Set to a non-0 value to run a fast train (for debugging purposes)
    validation_subset_size: 0  # Set to a non-0 value to run a fast validation (for debugging purposes)
model:
  class_path: algorithms.BaseDeepfakeDetectionModel
  init_args:
    model_name: "dinov2_vitl14_probe"
    optimizer:
      class_path: torch.optim.SGD
      init_args:
        lr: 0.001
        momentum: 0.9
        dampening: 0.0
        weight_decay: 0.0001
    scheduler: "OneCycleLR"
    model_input_size: 224
    classification_threshold: 0.5
    training_cropping_strategy: "resize"
    evaluation_cropping_strategy: "resize"
